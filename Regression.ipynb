{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-pis68esLde"
      },
      "outputs": [],
      "source": [
        "1. What is Simple Linear Regression?\n",
        "  - A model with exactly one explanatory variable is a simple linear regression\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  -   The key assumptions of Simple Linear Regression are linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c\n",
        "  - In the equation Y = mX + c, the coefficient 'm' represents the slope or gradient of the line, indicating its steepness and direction.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c\n",
        "  - In the equation Y = mX + c, the \"c\" represents the y-intercept, which is the point where the line crosses the y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression\n",
        "  - In simple linear regression, the slope (m) is calculated by dividing the covariance of x and y by the variance of x\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "   - The least squares method in simple linear regression aims to find the \"best-fit\" line for a dataset by minimizing the sum of the squared differences (errors) between the actual and predicted values, effectively finding the line that best represents the relationship between the variables.\n",
        "\n",
        " 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "   - In simple linear regression, the coefficient of determination (R²) represents the proportion of the variance in the dependent variable that is explained by the independent variable, ranging from 0 to 1, with higher values indicating a better model fit.\n",
        "\n",
        " 8. What is Multiple Linear Regression\n",
        "   - Multiple linear regression is a statistical technique that uses multiple independent variables to predict the outcome of a single dependent variable, assuming a linear relationship between them.\n",
        "\n",
        " 9. What is the main difference between Simple and Multiple Linear Regression\n",
        "   - The primary difference between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable: simple regression uses one, while multiple regression uses two or more\n",
        "\n",
        " 10. What are the key assumptions of Multiple Linear Regression\n",
        "   - The key assumptions of Multiple Linear Regression are linearity, independence of errors, homoscedasticity, multivariate normality of residuals, and no multicollinearity\n",
        "\n",
        " 11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "   - Heteroscedasticity, in the context of multiple linear regression, refers to the unequal variance of the error terms (residuals) across different levels of the independent variables. This violates a key assumption of the Ordinary Least Squares (OLS) regression model, leading to unreliable hypothesis testing and potentially biased standard errors\n",
        "\n",
        " 12.  How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "   - To improve a multiple linear regression model with high multicollinearity, you can remove one of the highly correlated variables, combine them into a single variable, or use dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
        "\n",
        " 13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "   - To use categorical variables in regression models, you can transform them into numerical data using techniques like one-hot encoding (also known as dummy encoding), label encoding, or binary encoding.\n",
        "\n",
        " 14. What is the role of interaction terms in Multiple Linear Regression\n",
        "   - In multiple linear regression, interaction terms reveal how the effect of one independent variable on the dependent variable changes depending on the value of another independent variable, going beyond simple additive effects.\n",
        "\n",
        " 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "   - In both simple and multiple linear regression, the intercept represents the predicted value of the dependent variable when all independent variables are zero, but in multiple regression, the intercept is the baseline value when all predictors are zero, while in simple regression, it's when the single predictor is zero.\n",
        "\n",
        " 16. What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "   - In regression analysis, the slope signifies the rate of change in the dependent variable for every one-unit change in the independent variable, and it directly impacts predictions by determining the direction and magnitude of the relationship.\n",
        "\n",
        " 17. How does the intercept in a regression model provide context for the relationship between variables\n",
        "   - In a regression model, the intercept (the \"y-intercept\" or \"constant term\") provides context for the relationship between variables by representing the predicted value of the dependent variable when all independent variables are zero, serving as a baseline or starting point.\n",
        "\n",
        " 18. What are the limitations of using R² as a sole measure of model performance\n",
        "   - R-squared, while useful, has limitations when used as the sole measure of model performance, as it doesn't indicate model correctness, reliability, or account for bias, and can be inflated by adding irrelevant variables.\n",
        "\n",
        " 19. How would you interpret a large standard error for a regression coefficient\n",
        "   - A large standard error for a regression coefficient suggests that the estimated coefficient is highly variable and less precise, meaning the true population value could be significantly different from the estimated value\n",
        "\n",
        " 20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "   - Heteroscedasticity, where error variance isn't constant, is identified in residual plots by a \"fan\" or \"cone\" shape, where the spread of residuals changes with the fitted values. Addressing it is crucial because it can lead to inaccurate regression model estimates and invalid statistical inferences.\n",
        "\n",
        " 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "   - When a multiple linear regression model shows a high R-squared but a low adjusted R-squared, it suggests that the model might be overfitting the data, meaning it's including many predictors that don't significantly improve the model's predictive power.\n",
        "\n",
        " 22. Why is it important to scale variables in Multiple Linear Regression\n",
        "   - Scaling variables in Multiple Linear Regression is important because it ensures all features contribute equally to the model, prevents larger-scale variables from dominating, and can improve the performance and interpretability of the model, especially when using algorithms like Gradient Descent or those sensitive to feature scale.\n",
        "\n",
        " 23. What is polynomial regression\n",
        "   - Polynomial regression is an extension of linear regression that models the relationship between a dependent variable and one or more independent variables using a polynomial function, allowing for capturing non-linear relationships that linear regression cannot.\n",
        "\n",
        " 24. How does polynomial regression differ from linear regression\n",
        "   - Polynomial regression extends linear regression by allowing for curved relationships between variables, fitting a polynomial equation to the data, while linear regression assumes a straight-line relationship.\n",
        "\n",
        " 25. When is polynomial regression used\n",
        "   - Polynomial regression is used when the relationship between the independent and dependent variables is not linear, meaning a straight line cannot accurately predict the outcomes, and a curve is a better fit.\n",
        "\n",
        " 26. What is the general equation for polynomial regression\n",
        "   - The general equation for polynomial regression, modeling a relationship between a dependent variable (y) and an independent variable (x) as a polynomial, is y = β₀ + β₁x + β₂x² + β₃x³ + ... + βₙxⁿ.\n",
        "\n",
        " 27. Can polynomial regression be applied to multiple variables\n",
        "   - Yes, polynomial regression can be applied to multiple variables\n",
        "\n",
        " 28. What are the limitations of polynomial regression\n",
        "   - Polynomial regression, while versatile for modeling non-linear relationships, has limitations including the risk of overfitting, poor extrapolation beyond the data range, and potential computational complexity with higher-degree polynomials.\n",
        "\n",
        " 29.  What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "   - To evaluate model fit and select the appropriate degree for a polynomial, you can use methods like visual inspection of plots, cross-validation with metrics like R-squared, and model comparison techniques such as ANOVA or information criteria (AIC, BIC).\n",
        "\n",
        " 30.  Why is visualization important in polynomial regression\n",
        "   - Visualization is crucial in polynomial regression because it allows you to visually assess the model's fit, identify potential overfitting or underfitting, and understand the relationship between variables, especially when dealing with non-linear relationships\n",
        "\n",
        " 31. How is polynomial regression implemented in Python\n",
        "   - The polynomial regression model is then trained by adjusting the coefficients of the polynomial terms to minimize the difference between the observed and predicted values of the dependent variable. The resulting equation can then be used to make predictions for new data.\n"
      ]
    }
  ]
}